{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import ezodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet Hey there! greet Hi greet Hey how may I Help you greet Hello bye Good bye bye Thank you. Take care good bye. request can I have your request number. request Can you please provide me your request number. request I need your request number to go ahead with your request \n"
     ]
    }
   ],
   "source": [
    "def read_ods(filename, sheet_no=0, header=0):\n",
    "    tab = ezodf.opendoc(filename=filename).sheets[sheet_no]\n",
    "    return pd.DataFrame({col[header].value:[x.value for x in col[header+1:]]\n",
    "                         for col in tab.columns()})\n",
    "x =  read_ods(filename = \"sentenceGenerator.ods\")\n",
    "sentences = x[\"act\"]+\" \"+ x[\"sentence\"]\n",
    "modSentence = \"\"\n",
    "for eachSentence in sentences:\n",
    "    modSentence += eachSentence + \" \"\n",
    "print(modSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text File and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '.': 1, 'ahead': 2, 'bye': 3, 'can': 4, 'care': 5, 'go': 6, 'good': 7, 'greet': 8, 'have': 9, 'hello': 10, 'help': 11, 'hey': 12, 'hi': 13, 'how': 14, 'i': 15, 'may': 16, 'me': 17, 'need': 18, 'number': 19, 'please': 20, 'provide': 21, 'request': 22, 'take': 23, 'thank': 24, 'there': 25, 'to': 26, 'with': 27, 'you': 28, 'your': 29}\n",
      "Total number of words:\t57\n",
      "Length of vocabulary:\t30\n"
     ]
    }
   ],
   "source": [
    "data_path = \"test_data_long.txt\"\n",
    "raw_text = modSentence.lower()\n",
    "# generate list of unique characters, but only include words and some punctuation marks\n",
    "pattern = re.compile('[a-z]+|\\!|\\n|\\.|,|;')\n",
    "all_words = re.findall(pattern, raw_text)\n",
    "\n",
    "unique_words = sorted(set(all_words))\n",
    "\n",
    "word_to_int = dict((c, i) for i, c in enumerate(unique_words))\n",
    "\n",
    "#print(unique_words)\n",
    "print(word_to_int)\n",
    "\n",
    "# later used to make outputs more readable by converting ints back to characters\n",
    "int_to_word = dict((i, c) for i, c in enumerate(unique_words))\n",
    "\n",
    "total_num_words = len(all_words)\n",
    "len_vocab = len(unique_words)\n",
    "\n",
    "print(\"Total number of words:\\t\" + str(total_num_words))\n",
    "print(\"Length of vocabulary:\\t\" + str(len_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "I will use a single hidden LSTM layer with 256 memory units and a dropout probability of 20%. The dense layer will use a softmax activation to output a probability prediction for each of the characters, between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "num_memory_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_memory_units, input_shape=(sequence_length, len_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temperature(predictions, temperature=1.0):\n",
    "\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    \n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_word(n=1, array=unique_words):\n",
    "\n",
    "    random_words = []\n",
    "    \n",
    "    random_indices = random.sample(range(0, len(array)), n)\n",
    "    \n",
    "    # in-place shuffle\n",
    "    random.shuffle(array)\n",
    "\n",
    "    # take the first n elements of the now randomized array\n",
    "    return array[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "weights_path = 'weights_epoch-27_loss-0.4866659641265869.hdf5'\n",
    "model.load_weights(weights_path)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> seed: \"request\" ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user_input = \"this is some string entered by a user this is some string entered by a user this is some string entered by a user\"\n",
    "user_input = \"request\"\n",
    "words_to_generate = 6\n",
    "seed_sentence = [\"\\n\"] * sequence_length\n",
    "\n",
    "if not random_seed:\n",
    "\n",
    "    pattern = re.compile('[a-z]+|\\!|\\n|\\.|,|;')\n",
    "    user_input_edit = user_input.lower()\n",
    "    user_input_edit = re.findall(pattern, user_input)\n",
    "\n",
    "    if (len(user_input_edit) > sequence_length):\n",
    "        # need to truncate\n",
    "        \n",
    "        seed_sentence = user_input_edit[:sequence_length]\n",
    "        \n",
    "    elif (len(user_input_edit) < sequence_length):\n",
    "        # need to pad\n",
    "        \n",
    "        # get number of elements missing in user input\n",
    "        missing_elems = sequence_length - len(user_input_edit)\n",
    "        \n",
    "        seed_sentence[0:missing_elems+1] = get_random_word(missing_elems)\n",
    "        seed_sentence[-(len(user_input_edit)):] = user_input_edit\n",
    "        \n",
    "    # check all words provided are in the vocabulary\n",
    "    for i, word in enumerate(seed_sentence):\n",
    "        # if it doesnt exist, replace it with one that does\n",
    "        if word not in word_to_int.keys():\n",
    "            seed_sentence[i] = get_random_word()[0]\n",
    "    \n",
    "    print('-> seed: \"' + user_input + '\" ...\\n')\n",
    "\n",
    "else:\n",
    "\n",
    "    # pick a random seed\n",
    "    random_index_start = np.random.randint(0, total_num_words - sequence_length - 1)\n",
    "    seed_sentence = all_words[random_index_start : random_index_start + sequence_length]\n",
    "    \n",
    "    print('-> seed: \"' + ' '.join(seed_sentence) + '\" ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i have your request number."
     ]
    }
   ],
   "source": [
    "for i in range(words_to_generate):\n",
    "        \n",
    "    x_input = np.zeros((1, sequence_length, len_vocab))\n",
    "    for t, word in enumerate(seed_sentence):\n",
    "        x_input[0, t, word_to_int[word]] = 1.\n",
    "\n",
    "    predictions = model.predict(x_input, verbose=0)[0]\n",
    "    predicted_word_index = add_temperature(predictions, 0.5)\n",
    "    predicted_word = int_to_word[predicted_word_index]\n",
    "\n",
    "    seed_sentence = seed_sentence[1:] + list([predicted_word])\n",
    "\n",
    "    if re.match('[a-z]', predicted_word):\n",
    "        sys.stdout.write(\" \" + predicted_word)\n",
    "    else:\n",
    "        sys.stdout.write(predicted_word)\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
