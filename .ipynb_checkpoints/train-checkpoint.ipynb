{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ezodf\n",
    "\n",
    "def read_ods(filename, sheet_no=0, header=0):\n",
    "    tab = ezodf.opendoc(filename=filename).sheets[sheet_no]\n",
    "    return pd.DataFrame({col[header].value:[x.value for x in col[header+1:]]\n",
    "                         for col in tab.columns()})\n",
    "x =  read_ods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load File and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:\t100440\n",
      "Length of vocabulary:\t5686\n"
     ]
    }
   ],
   "source": [
    "data_path = \"test_data_long.txt\"\n",
    "raw_text = open(data_path).read().lower()\n",
    "\n",
    "# generate list of unique characters, but only include words and some punctuation marks\n",
    "pattern = re.compile('[a-z]+|\\!|\\n|\\.|,|;')\n",
    "all_words = re.findall(pattern, raw_text)\n",
    "\n",
    "unique_words = sorted(set(all_words))\n",
    "\n",
    "word_to_int = dict((c, i) for i, c in enumerate(unique_words))\n",
    "\n",
    "# print(unique_words)\n",
    "# print(word_to_int)\n",
    "\n",
    "# later used to make outputs more readable by converting ints back to characters\n",
    "int_to_word = dict((i, c) for i, c in enumerate(unique_words))\n",
    "\n",
    "total_num_words = len(all_words)\n",
    "len_vocab = len(unique_words)\n",
    "\n",
    "print(\"Total number of words:\\t\" + str(total_num_words))\n",
    "print(\"Length of vocabulary:\\t\" + str(len_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Data from Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:\t33475\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 15 # max number of words to consider at a time.\n",
    "                    # this means that each trainig set (training pattern) will be comprised of 20 time steps\n",
    "step_window = 3\n",
    "\n",
    "# set up x and y\n",
    "# convert the words into integers\n",
    "x_data = [] # list of lists\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, total_num_words - sequence_length, step_window):\n",
    "    \n",
    "    # extract the first n words (length sequence_length): our \"x\"\n",
    "    sequence_in = all_words[i : i+sequence_length]\n",
    "    \n",
    "    # extract last word for this window: our \"y\" (target)\n",
    "    word_out = all_words[i+sequence_length]\n",
    "    \n",
    "    # print('\\nx: ' + str(sequence_in) + '\\n' + 'y: ' + word_out)\n",
    "    # print()\n",
    "        \n",
    "    # store corresponding integer for each character in the input sequence\n",
    "    x_data.append(sequence_in)\n",
    "    y_data.append(word_out)\n",
    "\n",
    "num_train_patters = len(x_data)\n",
    "print('Total patterns:\\t' + str(num_train_patters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-93a03f9c12f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_patters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_patters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# encode all data into one-hot vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.zeros((num_train_patters, sequence_length, len_vocab))\n",
    "y = np.zeros((num_train_patters, len_vocab))\n",
    "\n",
    "# encode all data into one-hot vectors\n",
    "for i, sentence in enumerate(x_data):\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[i, t, word_to_int[word]] = 1\n",
    "    y[i, word_to_int[y_data[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "num_memory_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(LSTM(num_memory_units, return_sequences=True, input_shape=(sequence_length, len_vocab)))\n",
    "model.add(LSTM(num_memory_units, input_shape=(sequence_length, len_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(num_memory_units))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(len_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_temperature(predictions, temperature=1.0):\n",
    "\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    \n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- iteration 1/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 3.5528\n",
      "Loss improved from inf to 3.55284714699. Saving weights.\n",
      "\n",
      "-> seed: \"worried look on his usually\" ...\n",
      "\n",
      " usually cheerful cheerful.. his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a, he he he over the the second second before before it it to to... his he he he a a,\n",
      "\n",
      "---------- iteration 2/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.9307\n",
      "Loss improved from 3.55284714699 to 2.93071365356. Saving weights.\n",
      "\n",
      "-> seed: \"held a letter , which\" ...\n",
      "\n",
      " he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he\n",
      "\n",
      "---------- iteration 3/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 3.2152\n",
      "\n",
      "-> seed: \"in his chair with a\" ...\n",
      "\n",
      " worried worried on usually usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with worried worried on on usually usually cheerful cheerful cheerful cheerful his his his with with\n",
      "\n",
      "---------- iteration 4/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.7556\n",
      "Loss improved from 2.93071365356 to 1.75558710098. Saving weights.\n",
      "\n",
      "-> seed: \"worried look on his usually\" ...\n",
      "\n",
      " cheerful cheerful cheerful cheerful his his his with with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually usually cheerful cheerful.. his his his with worried worried worried on usually usually\n",
      "\n",
      "---------- iteration 5/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7028\n",
      "Loss improved from 1.75558710098 to 0.702775657177. Saving weights.\n",
      "\n",
      "-> seed: \"cheerful face . in his\" ...\n",
      "\n",
      " his he a a.,, on he over over the second second before before it it it to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second second before before before it it to to.... his he he he,,, over over over over second\n",
      "\n",
      "---------- iteration 6/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.5092\n",
      "Loss improved from 0.702775657177 to 0.509203493595. Saving weights.\n",
      "\n",
      "-> seed: \"standish lay back in his\" ...\n",
      "\n",
      " with with worried worried worried on usually usually usually cheerful cheerful... his with he he worried worried, the usually usually usually the... his he he he a a the,, usually usually over over the. second second second before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before it it to to.... he he he he, the the the the second second before before\n",
      "\n",
      "---------- iteration 7/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2851\n",
      "Loss improved from 0.509203493595 to 0.285060673952. Saving weights.\n",
      "\n",
      "-> seed: \"\n",
      " cheerful face . in\" ...\n",
      "\n",
      " his his he he a a a,, he he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he he over,, second second second over over it it it second... his it he he he a,,, he over over over second second second before before it it to to.... he he he\n",
      "\n",
      "---------- iteration 8/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.2156\n",
      "Loss improved from 0.285060673952 to 0.215642571449. Saving weights.\n",
      "\n",
      "-> seed: \"cheerful face . in his\" ...\n",
      "\n",
      " his he he worried. a the, usually he usually before over.,. to he he over he the the the second second before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he over the the the the before before before it it to to.... he he he he\n",
      "\n",
      "---------- iteration 9/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0872\n",
      "Loss improved from 0.215642571449 to 0.0871827974916. Saving weights.\n",
      "\n",
      "-> seed: \"second time before tossing it\" ...\n",
      "\n",
      " it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it it to to.... he he he he, the the the the before before before it\n",
      "\n",
      "---------- iteration 10/10 ----------\n",
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0428\n",
      "Loss improved from 0.0871827974916 to 0.0428249575198. Saving weights.\n",
      "\n",
      "-> seed: \". in his hand he\" ...\n",
      "\n",
      " he a a,,, he over over over the the second second before before it it to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to.... he he he he over the the the second before before before it to to to\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 50\n",
    "batch_size = 128\n",
    "words_to_generate = 300\n",
    "\n",
    "prev_loss = math.inf\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    print('\\n' + '-'*10 + ' epoch ' + str(i+1) + '/' + str(num_iterations) + ' ' + '-'*10)\n",
    "        \n",
    "    history = model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "    \n",
    "    curr_loss = history.history['loss'][0]\n",
    "    loss_history.append(curr_loss)\n",
    "    \n",
    "    # save weights if loss improves\n",
    "    if (curr_loss < prev_loss):\n",
    "        print(\"Loss improved from \" + str(prev_loss) + \" to \" + str(curr_loss) + \". Saving weights.\")\n",
    "        model.save_weights('weights_epoch-{}_loss-{}.hdf5'.format(i, curr_loss))\n",
    "        prev_loss = curr_loss\n",
    "    \n",
    "    start_index = random.randint(0, total_num_words - sequence_length - 1)\n",
    "    # start_index = 0\n",
    "\n",
    "    seed_sentence = all_words[start_index : start_index + sequence_length]\n",
    "\n",
    "    print('\\n-> seed: \"' + ' '.join(seed_sentence) + '\" ...\\n')\n",
    "\n",
    "    for i in range(words_to_generate):\n",
    "        \n",
    "        x_input = np.zeros((1, sequence_length, len_vocab))\n",
    "        for t, word in enumerate(seed_sentence):\n",
    "            x_input[0, t, word_to_int[word]] = 1.\n",
    "\n",
    "        predictions = model.predict(x_input, verbose=0)[0]\n",
    "        \n",
    "        if i == num_iterations-1:\n",
    "            final_predicted = predictions\n",
    "        \n",
    "        # predicted_word_index = add_temperature(predictions, 0.5)\n",
    "        predicted_word_index = np.argmax(predictions)\n",
    "        predicted_word = int_to_word[predicted_word_index]\n",
    "\n",
    "        seed_sentence = seed_sentence[1:] + list([predicted_word])\n",
    "\n",
    "        if re.match('[a-z]', predicted_word):\n",
    "            sys.stdout.write(\" \" + predicted_word)\n",
    "        else:\n",
    "            sys.stdout.write(predicted_word)\n",
    "    \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 266us/step\n",
      "loss:  0.0209426973015\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(x, y, batch_size=batch_size, verbose=1)\n",
    "print(\"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss history:\n",
      "[3.552847146987915, 2.9307136535644531, 3.2151951789855957, 1.755587100982666, 0.70277565717697144, 0.50920349359512329, 0.28506067395210266, 0.21564257144927979, 0.087182797491550446, 0.042824957519769669]\n"
     ]
    }
   ],
   "source": [
    "print('loss history:')\n",
    "print(loss_history)\n",
    "\n",
    "# plt.figure(figsize=(15,8))\n",
    "# plt.rc('font', size=20)\n",
    "# plt.plot(loss_history, lw=3, c='orange')\n",
    "# plt.title('Cross Entropy Loss of LSTM Model over Epoch Iterations', fontsize=25)\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.savefig(\"loss.png\")\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 5, 256)            299008    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 35)                4515      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 35)                0         \n",
      "=================================================================\n",
      "Total params: 500,643\n",
      "Trainable params: 500,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
